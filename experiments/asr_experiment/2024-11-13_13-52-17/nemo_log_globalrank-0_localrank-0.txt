[NeMo W 2024-11-13 13:52:13 nemo_logging:349] /home/giolinux/miniconda3/envs/NLP_project/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:254: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
      def forward(
    
[NeMo W 2024-11-13 13:52:13 nemo_logging:349] /home/giolinux/miniconda3/envs/NLP_project/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:265: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
      def backward(ctx, grad_output):
    
[NeMo W 2024-11-13 13:52:13 nemo_logging:349] /home/giolinux/miniconda3/envs/NLP_project/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:325: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
      def forward(
    
[NeMo W 2024-11-13 13:52:13 nemo_logging:349] /home/giolinux/miniconda3/envs/NLP_project/lib/python3.10/site-packages/megatron/core/tensor_parallel/layers.py:360: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
      def backward(ctx, grad_output):
    
[NeMo I 2024-11-13 13:52:17 exp_manager:396] Experiments will be logged at experiments/asr_experiment/2024-11-13_13-52-17
[NeMo I 2024-11-13 13:52:17 exp_manager:842] TensorboardLogger has been set up
[NeMo I 2024-11-13 13:52:20 collections:445] Filtered duration for loading collection is  0.00 hours.
[NeMo I 2024-11-13 13:52:20 collections:446] Dataset loaded with 100 items, total duration of  0.02 hours.
[NeMo I 2024-11-13 13:52:20 collections:448] # 100 files loaded accounting to # 14 labels
[NeMo I 2024-11-13 13:52:20 collections:445] Filtered duration for loading collection is  0.00 hours.
[NeMo I 2024-11-13 13:52:20 collections:446] Dataset loaded with 26 items, total duration of  0.01 hours.
[NeMo I 2024-11-13 13:52:20 collections:448] # 26 files loaded accounting to # 12 labels
[NeMo I 2024-11-13 13:52:20 features:289] PADDING: 16
[NeMo I 2024-11-13 13:52:20 collections:445] Filtered duration for loading collection is  0.00 hours.
[NeMo I 2024-11-13 13:52:20 collections:446] Dataset loaded with 100 items, total duration of  0.02 hours.
[NeMo I 2024-11-13 13:52:20 collections:448] # 100 files loaded accounting to # 14 labels
[NeMo I 2024-11-13 13:52:20 collections:445] Filtered duration for loading collection is  0.00 hours.
[NeMo I 2024-11-13 13:52:20 collections:446] Dataset loaded with 26 items, total duration of  0.01 hours.
[NeMo I 2024-11-13 13:52:20 collections:448] # 26 files loaded accounting to # 12 labels
[NeMo I 2024-11-13 13:52:21 modelPT:723] Optimizer config = Adam (
    Parameter Group 0
        amsgrad: False
        betas: (0.9, 0.999)
        capturable: False
        differentiable: False
        eps: 1e-08
        foreach: None
        fused: None
        lr: 0.0012424318997362363
        maximize: False
        weight_decay: 0.0001
    )
[NeMo I 2024-11-13 13:52:21 lr_scheduler:772] Scheduler not initialized as no `sched` config supplied to setup_optimizer()
[NeMo W 2024-11-13 13:52:25 nemo_logging:349] /home/giolinux/miniconda3/envs/NLP_project/lib/python3.10/site-packages/nemo/collections/asr/parts/preprocessing/features.py:417: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
      with torch.cuda.amp.autocast(enabled=False):
    
[NeMo W 2024-11-13 13:52:26 nemo_logging:349] /home/giolinux/miniconda3/envs/NLP_project/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
      rank_zero_warn(
    
[NeMo W 2024-11-13 13:52:26 nemo_logging:349] /home/giolinux/miniconda3/envs/NLP_project/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
      warning_cache.warn(
    
